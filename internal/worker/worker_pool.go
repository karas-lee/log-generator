package worker

import (
	"context"
	"fmt"
	"log-generator/internal/config"
	"runtime"
	"runtime/debug"
	"sync"
	"sync/atomic"
	"time"
	
	"github.com/shirou/gopsutil/v3/cpu"
)

const (
	// PRD ëª…ì„¸: 400ë§Œ EPS = 40ê°œ ì›Œì»¤ Ã— 10ë§Œ EPS
	TOTAL_WORKERS = 40
	FIRST_PORT = 10514  // ë†’ì€ í¬íŠ¸ ì‚¬ìš© (ê¶Œí•œ ë¬¸ì œ íšŒí”¼)
	LAST_PORT = 10553   // 10514 + 39 = 10553
	MAX_WORKERS = 200   // ìµœëŒ€ ì›Œì»¤ ìˆ˜ (4M í”„ë¡œíŒŒì¼ìš©)
	MAX_PORT = 10714    // 10514 + 200 = 10714
)

// WorkerPoolMetrics - ì›Œì»¤ í’€ ì „ì²´ ë©”íŠ¸ë¦­
type WorkerPoolMetrics struct {
	TotalEPS        int64                    `json:"total_eps"`
	TotalSent       int64                    `json:"total_sent"`
	TotalErrors     int64                    `json:"total_errors"`
	ActiveWorkers   int                      `json:"active_workers"`
	AverageEPS      int64                    `json:"average_eps"`
	PacketLossRate  float64                  `json:"packet_loss_rate"`
	WorkerMetrics   map[int]WorkerMetrics    `json:"worker_metrics"`
	SystemMetrics   SystemMetrics            `json:"system_metrics"`
	LastUpdate      time.Time                `json:"last_update"`
}

// SystemMetrics - ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­
type SystemMetrics struct {
	CPUUsagePercent    float64 `json:"cpu_usage_percent"`
	MemoryUsageMB      float64 `json:"memory_usage_mb"`
	GoroutineCount     int     `json:"goroutine_count"`
	GCPauseMs          float64 `json:"gc_pause_ms"`
	NetworkTxMBps      float64 `json:"network_tx_mbps"`
	NetworkTxPackets   int64   `json:"network_tx_packets"`
	NetworkRxPackets   int64   `json:"network_rx_packets"`
	NetworkTxBytes     int64   `json:"network_tx_bytes"`
	NetworkRxBytes     int64   `json:"network_rx_bytes"`
}

// WorkerPool - ê³ ì„±ëŠ¥ ì›Œì»¤ í’€ ê´€ë¦¬ì (í”„ë¡œíŒŒì¼ ê¸°ë°˜)
type WorkerPool struct {
	// ì›Œì»¤ ê´€ë¦¬
	workers         []*UDPWorker
	workerCount     int
	targetHost      string
	
	// í”„ë¡œíŒŒì¼ ì„¤ì •
	profile         *config.EPSProfile
	
	// ë©”íŠ¸ë¦­ ìˆ˜ì§‘
	metricsChannel  chan WorkerMetrics
	poolMetrics     atomic.Value  // WorkerPoolMetrics ì €ì¥
	
	// ìƒíƒœ ê´€ë¦¬
	isRunning       atomic.Bool
	startTime       time.Time
	
	// ë™ê¸°í™”
	ctx             context.Context
	cancel          context.CancelFunc
	wg              sync.WaitGroup
	mutex           sync.RWMutex
	
	// ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
	epsHistory      []int64    // EPS ì´ë ¥ (ìµœê·¼ 300ì´ˆ)
	historyIndex    int
	lastHistoryTime time.Time
	
	// ìë™ ì¡°ì ˆ ê¸°ëŠ¥
	autoTuning      bool
	targetEPS       int64
	tuningEnabled   atomic.Bool
}

// NewWorkerPool - ì›Œì»¤ í’€ ìƒì„± ë° ì´ˆê¸°í™”
func NewWorkerPool(targetHost string) *WorkerPool {
	ctx, cancel := context.WithCancel(context.Background())
	
	// ê¸°ë³¸ í”„ë¡œíŒŒì¼ (4M)
	defaultProfile := config.EPSProfiles["4m"]
	
	pool := &WorkerPool{
		workers:        make([]*UDPWorker, 0, TOTAL_WORKERS),
		targetHost:     targetHost,
		profile:        defaultProfile,
		metricsChannel: make(chan WorkerMetrics, TOTAL_WORKERS*2), // ë²„í¼ í¬ê¸° ì—¬ìœ 
		ctx:            ctx,
		cancel:         cancel,
		epsHistory:     make([]int64, 300), // 5ë¶„ê°„ ì´ë ¥
		targetEPS:      int64(defaultProfile.TargetEPS),
		autoTuning:     true,
	}
	
	// ì´ˆê¸° ë©”íŠ¸ë¦­ ì„¤ì •
	pool.poolMetrics.Store(WorkerPoolMetrics{
		WorkerMetrics: make(map[int]WorkerMetrics),
		LastUpdate:    time.Now(),
	})
	
	return pool
}

// NewWorkerPoolWithProfile - í”„ë¡œíŒŒì¼ ê¸°ë°˜ ì›Œì»¤ í’€ ìƒì„±
func NewWorkerPoolWithProfile(targetHost string, profile *config.EPSProfile) *WorkerPool {
	ctx, cancel := context.WithCancel(context.Background())
	
	// í”„ë¡œíŒŒì¼ ê¸°ë°˜ ìµœì í™” ì ìš©
	if profile.GOGC > 0 {
		debug.SetGCPercent(profile.GOGC)
	}
	if profile.MemoryLimit > 0 {
		debug.SetMemoryLimit(profile.MemoryLimit)
	}
	
	pool := &WorkerPool{
		workers:        make([]*UDPWorker, 0, profile.WorkerCount),
		targetHost:     targetHost,
		profile:        profile,
		metricsChannel: make(chan WorkerMetrics, profile.WorkerCount*2),
		ctx:            ctx,
		cancel:         cancel,
		epsHistory:     make([]int64, 300),
		targetEPS:      int64(profile.TargetEPS),
		autoTuning:     false, // í”„ë¡œíŒŒì¼ ëª¨ë“œì—ì„œëŠ” ìë™ íŠœë‹ ë¹„í™œì„±í™”
	}
	
	// ì´ˆê¸° ë©”íŠ¸ë¦­ ì„¤ì •
	pool.poolMetrics.Store(WorkerPoolMetrics{
		WorkerMetrics: make(map[int]WorkerMetrics),
		LastUpdate:    time.Now(),
	})
	
	return pool
}

// Initialize - í”„ë¡œíŒŒì¼ ê¸°ë°˜ ì›Œì»¤ ì´ˆê¸°í™”
func (wp *WorkerPool) Initialize() error {
	wp.mutex.Lock()
	defer wp.mutex.Unlock()
	
	if wp.isRunning.Load() {
		return fmt.Errorf("ì›Œì»¤ í’€ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
	}
	
	// í”„ë¡œíŒŒì¼ì— ë”°ë¥¸ ì›Œì»¤ ìˆ˜ ê²°ì •
	workerCount := wp.profile.WorkerCount
	// ìµœëŒ€ ì›Œì»¤ ìˆ˜ ì œí•œ í™•ì¥
	if workerCount > MAX_WORKERS {
		workerCount = MAX_WORKERS
	}
	
	// ì›Œì»¤ ìƒì„±
	for i := 0; i < workerCount; i++ {
		workerID := i + 1
		port := FIRST_PORT + i
		
		// í”„ë¡œíŒŒì¼ ì„¤ì •ìœ¼ë¡œ ì›Œì»¤ ìƒì„±
		worker, err := NewUDPWorkerWithConfig(workerID, port, wp.targetHost, wp.metricsChannel, 
			wp.profile.BatchSize, wp.profile.TickerInterval)
		if err != nil {
			return fmt.Errorf("ì›Œì»¤ %d ìƒì„± ì‹¤íŒ¨: %v", workerID, err)
		}
		
		// ë²„í¼ í¬ê¸° ì„¤ì •
		if wp.profile.SendBufferSize > 0 {
			worker.SetBufferSizes(wp.profile.SendBufferSize*1024, wp.profile.ReceiveBufferSize*1024)
		}
		
		// ì›Œì»¤ë‹¹ ëª©í‘œ EPS ì„¤ì • (ì „ì²´ ëª©í‘œ / ì›Œì»¤ ìˆ˜)
		workerTargetEPS := int64(wp.profile.TargetEPS / workerCount)
		worker.SetTargetEPS(workerTargetEPS)
		
		// ì •ë°€ë„ ëª¨ë“œ ì„¤ì •
		if wp.profile.PrecisionMode != "" {
			worker.SetPrecisionMode(wp.profile.PrecisionMode)
		}
		
		wp.workers = append(wp.workers, worker)
	}
	
	wp.workerCount = len(wp.workers)
	_ = int64(wp.profile.TargetEPS / workerCount)  // workerTargetEPS
	
	// ì •ë°€ë„ ëª¨ë“œ í‘œì‹œ
	precisionMode := wp.profile.PrecisionMode
	if precisionMode == "" {
		precisionMode = "medium"
	}
	modeDescription := map[string]string{
		"high": "ë†’ì€ ì •ë°€ë„ (ì˜¤ì°¨ <1%)",
		"medium": "ì¤‘ê°„ ì •ë°€ë„ (ì˜¤ì°¨ <5%)",
		"performance": "ì„±ëŠ¥ ìš°ì„  (ì˜¤ì°¨ <10%)",
	}
	fmt.Printf("  ğŸ¯ Adaptive Rate Control í™œì„±í™” - %s ëª¨ë“œ: %s\n", precisionMode, modeDescription[precisionMode])
	
	return nil
}

// Start - ì›Œì»¤ í’€ ì‹œì‘ (400ë§Œ EPS ë‹¬ì„± ì‹œì‘)
func (wp *WorkerPool) Start() error {
	if !wp.isRunning.CompareAndSwap(false, true) {
		return fmt.Errorf("ì›Œì»¤ í’€ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
	}
	
	wp.startTime = time.Now()
	
	// ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì‹œì‘
	wp.wg.Add(1)
	go wp.metricsAggregator()
	
	// ìë™ íŠœë‹ ì‹œì‘
	if wp.autoTuning {
		wp.wg.Add(1)
		go wp.autoTuner()
	}
	
	// ëª¨ë“  ì›Œì»¤ ì‹œì‘
	successCount := int32(0)
	failCount := int32(0)
	
	for i, worker := range wp.workers {
		wp.wg.Add(1)
		go func(w *UDPWorker, index int) {
			defer wp.wg.Done()
			
			err := w.Start(wp.ctx)
			if err != nil {
				atomic.AddInt32(&failCount, 1)
			} else {
				atomic.AddInt32(&successCount, 1)
			}
		}(worker, i)
		
		// ì›Œì»¤ ì‹œì‘ ê°„ê²© ì¶•ì†Œ (ì´ì œ í¬íŠ¸ ë°”ì¸ë”© ê²½í•©ì´ ì—†ìŒ)
		time.Sleep(time.Millisecond * 2)
	}
	
	// ì›Œì»¤ ì‹œì‘ ìƒíƒœ í™•ì¸
	time.Sleep(100 * time.Millisecond)
	success := atomic.LoadInt32(&successCount)
	fail := atomic.LoadInt32(&failCount)
	_ = success
	_ = fail
	
	return nil
}

// metricsAggregator - ì›Œì»¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì§‘ê³„
func (wp *WorkerPool) metricsAggregator() {
	defer wp.wg.Done()
	
	ticker := time.NewTicker(time.Second)
	defer ticker.Stop()
	
	workerMetricsMap := make(map[int]WorkerMetrics)
	
	for {
		select {
		case <-wp.ctx.Done():
			return
		case <-ticker.C:
			// ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
			sysMetrics := wp.collectSystemMetrics()
			
			// ì›Œì»¤ ë©”íŠ¸ë¦­ ì§‘ê³„
			var totalEPS, totalSent, totalErrors int64
			var activeWorkers int
			var totalPacketLoss float64
			
			for _, worker := range wp.workers {
				if worker.IsRunning() {
					activeWorkers++
					totalEPS += worker.GetCurrentEPS()
					totalSent += worker.GetTotalSent()
				}
			}
			
			// í‰ê·  EPS ê³„ì‚°
			var averageEPS int64
			if activeWorkers > 0 {
				averageEPS = totalEPS / int64(activeWorkers)
			}
			
			// EPS ì´ë ¥ ì—…ë°ì´íŠ¸
			wp.updateEPSHistory(totalEPS)
			
			// í’€ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
			poolMetrics := WorkerPoolMetrics{
				TotalEPS:       totalEPS,
				TotalSent:      totalSent,
				TotalErrors:    totalErrors,
				ActiveWorkers:  activeWorkers,
				AverageEPS:     averageEPS,
				PacketLossRate: totalPacketLoss / float64(activeWorkers),
				WorkerMetrics:  workerMetricsMap,
				SystemMetrics:  sysMetrics,
				LastUpdate:     time.Now(),
			}
			
			wp.poolMetrics.Store(poolMetrics)
			
			// ì„±ëŠ¥ ë¡œê·¸ ì¶œë ¥ (1ë¶„ë§ˆë‹¤)
			if time.Since(wp.startTime).Seconds() > 0 && int(time.Since(wp.startTime).Seconds())%60 == 0 {
				wp.printPerformanceLog(poolMetrics)
			}
			
		case workerMetric := <-wp.metricsChannel:
			workerMetricsMap[workerMetric.WorkerID] = workerMetric
		}
	}
}

// collectSystemMetrics - ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
func (wp *WorkerPool) collectSystemMetrics() SystemMetrics {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	
	// ì‹¤ì œ CPU ì‚¬ìš©ë¥  ê°€ì ¸ì˜¤ê¸°
	cpuPercent := 0.0
	if percents, err := cpu.Percent(100*time.Millisecond, false); err == nil && len(percents) > 0 {
		cpuPercent = percents[0]
	}
	
	// ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°€ì ¸ì˜¤ê¸°
	// SysëŠ” Go runtimeì´ OSë¡œë¶€í„° í• ë‹¹ë°›ì€ ì „ì²´ ë©”ëª¨ë¦¬
	memoryMB := float64(m.Sys) / 1024 / 1024
	
	// ë„¤íŠ¸ì›Œí¬ íŒ¨í‚· ì •ë³´ ìˆ˜ì§‘
	var totalTxPackets int64
	var totalRxPackets int64
	var totalTxBytes int64
	var totalRxBytes int64
	
	// í˜„ì¬ ë©”íŠ¸ë¦­ì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
	currentMetrics := wp.GetMetrics()
	totalTxPackets = currentMetrics.TotalSent
	totalTxBytes = currentMetrics.TotalSent * 512 // í‰ê·  íŒ¨í‚· í¬ê¸° ì¶”ì • (512 bytes)
	
	// ë„¤íŠ¸ì›Œí¬ ì²˜ë¦¬ëŸ‰ ê³„ì‚° (Mbps)
	elapsedSeconds := time.Since(wp.startTime).Seconds()
	var networkTxMBps float64
	if elapsedSeconds > 0 {
		networkTxMBps = float64(totalTxBytes) * 8 / 1024 / 1024 / elapsedSeconds
	}
	
	return SystemMetrics{
		CPUUsagePercent:  cpuPercent,
		MemoryUsageMB:    memoryMB,
		GoroutineCount:   runtime.NumGoroutine(),
		GCPauseMs:        float64(m.PauseNs[(m.NumGC+255)%256]) / 1000000,
		NetworkTxMBps:    networkTxMBps,
		NetworkTxPackets: totalTxPackets,
		NetworkRxPackets: totalRxPackets,
		NetworkTxBytes:   totalTxBytes,
		NetworkRxBytes:   totalRxBytes,
	}
}

func (wp *WorkerPool) getCPUUsage() float64 {
	// ê°„ë‹¨í•œ CPU ì‚¬ìš©ë¥  ì¶”ì •
	// ì‹¤ì œë¡œëŠ” ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
	currentMetrics := wp.GetMetrics()
	if wp.targetEPS == 0 {
		return 0
	}
	
	targetAchievementRate := float64(currentMetrics.TotalEPS) / float64(wp.targetEPS)
	
	// CPU ì‚¬ìš©ë¥  ì¶”ì •: ë‹¬ì„±ë¥  * ì›Œì»¤ìˆ˜ * ê¸°ë³¸ ì‚¬ìš©ë¥ 
	cpuUsage := targetAchievementRate * float64(wp.workerCount) * 2.0 // ì›Œì»¤ë‹¹ ì•½ 2% CPU
	
	// ìµœëŒ€ 100%ë¡œ ì œí•œ
	if cpuUsage > 100 {
		cpuUsage = 100
	}
	
	return cpuUsage
}

// updateEPSHistory - EPS ì´ë ¥ ì—…ë°ì´íŠ¸
func (wp *WorkerPool) updateEPSHistory(currentEPS int64) {
	now := time.Now()
	if now.Sub(wp.lastHistoryTime) >= time.Second {
		wp.epsHistory[wp.historyIndex] = currentEPS
		wp.historyIndex = (wp.historyIndex + 1) % len(wp.epsHistory)
		wp.lastHistoryTime = now
	}
}

// autoTuner - ìë™ ì„±ëŠ¥ íŠœë‹ (ì‹¤í—˜ì  ê¸°ëŠ¥)
func (wp *WorkerPool) autoTuner() {
	defer wp.wg.Done()
	
	if !wp.tuningEnabled.Load() {
		return
	}
	
	ticker := time.NewTicker(time.Second * 30) // 30ì´ˆë§ˆë‹¤ íŠœë‹
	defer ticker.Stop()
	
	for {
		select {
		case <-wp.ctx.Done():
			return
		case <-ticker.C:
			metrics := wp.GetMetrics()
			
			// ëª©í‘œ EPSì˜ 95% ë¯¸ë§Œì¸ ê²½ìš° ìµœì í™” ì‹œë„
			if metrics.TotalEPS < (wp.targetEPS * 95 / 100) {
				wp.performAutoTuning(metrics)
			}
		}
	}
}

func (wp *WorkerPool) performAutoTuning(metrics WorkerPoolMetrics) {
	// CPU ì‚¬ìš©ë¥ ì´ ë‚®ìœ¼ë©´ ì›Œì»¤ ì¦ê°€ ì‹œë„
	if metrics.SystemMetrics.CPUUsagePercent < 60 {
		fmt.Printf("ğŸ”§ ìë™ íŠœë‹: CPU ì‚¬ìš©ë¥  ë‚®ìŒ (%.1f%%), ì„±ëŠ¥ í–¥ìƒ ì‹œë„\n", 
			metrics.SystemMetrics.CPUUsagePercent)
	}
	
	// ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìœ¼ë©´ GC ê°•ì œ ì‹¤í–‰
	if metrics.SystemMetrics.MemoryUsageMB > 10*1024 { // 10GB
		runtime.GC()
		fmt.Printf("ğŸ”§ ìë™ íŠœë‹: ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰ (%.1f MB)\n", 
			metrics.SystemMetrics.MemoryUsageMB)
	}
}

// printPerformanceLog - ì„±ëŠ¥ ë¡œê·¸ ì¶œë ¥
func (wp *WorkerPool) printPerformanceLog(metrics WorkerPoolMetrics) {
	// Performance logging disabled to reduce overhead
	_ = metrics
}

// Stop - ì›Œì»¤ í’€ ì •ì§€
func (wp *WorkerPool) Stop() error {
	if !wp.isRunning.CompareAndSwap(true, false) {
		return fmt.Errorf("ì›Œì»¤ í’€ì´ ì‹¤í–‰ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤")
	}
	
	// ì·¨ì†Œ ì‹ í˜¸ ì „ì†¡
	wp.cancel()
	
	// ëª¨ë“  ì›Œì»¤ ì •ì§€
	for _, worker := range wp.workers {
		err := worker.Stop()
		if err != nil {
			// Silently handle error
			_ = err
		}
	}
	
	// ê³ ë£¨í‹´ ì •ë¦¬ ëŒ€ê¸°
	wp.wg.Wait()
	
	// ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸
	finalMetrics := wp.GetMetrics()
	_ = finalMetrics
	
	return nil
}

// GetMetrics - í˜„ì¬ ë©”íŠ¸ë¦­ ë°˜í™˜ (thread-safe copy)
func (wp *WorkerPool) GetMetrics() WorkerPoolMetrics {
	if value := wp.poolMetrics.Load(); value != nil {
		original := value.(WorkerPoolMetrics)
		
		// WorkerMetrics ë§µì˜ ê¹Šì€ ë³µì‚¬ ìƒì„±
		workerMetricsCopy := make(map[int]WorkerMetrics)
		for id, metric := range original.WorkerMetrics {
			workerMetricsCopy[id] = metric
		}
		
		// ìƒˆë¡œìš´ êµ¬ì¡°ì²´ ìƒì„±í•˜ì—¬ ë°˜í™˜
		return WorkerPoolMetrics{
			TotalEPS:       original.TotalEPS,
			TotalSent:      original.TotalSent,
			TotalErrors:    original.TotalErrors,
			ActiveWorkers:  original.ActiveWorkers,
			AverageEPS:     original.AverageEPS,
			PacketLossRate: original.PacketLossRate,
			WorkerMetrics:  workerMetricsCopy,
			SystemMetrics:  original.SystemMetrics,
			LastUpdate:     original.LastUpdate,
		}
	}
	
	return WorkerPoolMetrics{
		WorkerMetrics: make(map[int]WorkerMetrics),
		LastUpdate:    time.Now(),
	}
}

// GetEPSHistory - EPS ì´ë ¥ ë°˜í™˜ (ëª¨ë‹ˆí„°ë§ìš©)
func (wp *WorkerPool) GetEPSHistory() []int64 {
	wp.mutex.RLock()
	defer wp.mutex.RUnlock()
	
	history := make([]int64, len(wp.epsHistory))
	copy(history, wp.epsHistory)
	return history
}

// IsRunning - ì‹¤í–‰ ìƒíƒœ í™•ì¸
func (wp *WorkerPool) IsRunning() bool {
	return wp.isRunning.Load()
}

// GetWorkerCount - ì›Œì»¤ ìˆ˜ ë°˜í™˜
func (wp *WorkerPool) GetWorkerCount() int {
	return wp.workerCount
}

// GetProfile - í˜„ì¬ í”„ë¡œíŒŒì¼ ë°˜í™˜
func (wp *WorkerPool) GetProfile() *config.EPSProfile {
	return wp.profile
}

// SetProfile - í”„ë¡œíŒŒì¼ ë³€ê²½ (ì¬ì‹œì‘ í•„ìš”)
func (wp *WorkerPool) SetProfile(profile *config.EPSProfile) error {
	if wp.isRunning.Load() {
		return fmt.Errorf("ì›Œì»¤ í’€ ì‹¤í–‰ ì¤‘ì—ëŠ” í”„ë¡œíŒŒì¼ì„ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
	}
	
	wp.profile = profile
	wp.targetEPS = int64(profile.TargetEPS)
	
	// í”„ë¡œíŒŒì¼ ê¸°ë°˜ ì‹œìŠ¤í…œ ìµœì í™”
	if profile.GOGC > 0 {
		debug.SetGCPercent(profile.GOGC)
	}
	if profile.MemoryLimit > 0 {
		debug.SetMemoryLimit(profile.MemoryLimit)
	}
	
	return nil
}

// EnableAutoTuning - ìë™ íŠœë‹ í™œì„±í™”/ë¹„í™œì„±í™”
func (wp *WorkerPool) EnableAutoTuning(enabled bool) {
	wp.tuningEnabled.Store(enabled)
}

// formatNumber - ìˆ«ì í¬ë§·íŒ… (ê°€ë…ì„± í–¥ìƒ)
func formatNumber(n int64) string {
	if n >= 1000000 {
		return fmt.Sprintf("%.2fM", float64(n)/1000000)
	} else if n >= 1000 {
		return fmt.Sprintf("%.1fK", float64(n)/1000)
	}
	return fmt.Sprintf("%d", n)
}

// strings.Repeat êµ¬í˜„
func repeatString(s string, count int) string {
	result := ""
	for i := 0; i < count; i++ {
		result += s
	}
	return result
}