package worker

import (
	"context"
	"fmt"
	"log-generator/internal/config"
	"runtime"
	"runtime/debug"
	"strings"
	"sync"
	"sync/atomic"
	"time"
)

const (
	// PRD ëª…ì„¸: 400ë§Œ EPS = 40ê°œ ì›Œì»¤ Ã— 10ë§Œ EPS
	TOTAL_WORKERS = 40
	FIRST_PORT = 10514  // ë†’ì€ í¬íŠ¸ ì‚¬ìš© (ê¶Œí•œ ë¬¸ì œ íšŒí”¼)
	LAST_PORT = 10553   // 10514 + 39 = 10553
)

// WorkerPoolMetrics - ì›Œì»¤ í’€ ì „ì²´ ë©”íŠ¸ë¦­
type WorkerPoolMetrics struct {
	TotalEPS        int64                    `json:"total_eps"`
	TotalSent       int64                    `json:"total_sent"`
	TotalErrors     int64                    `json:"total_errors"`
	ActiveWorkers   int                      `json:"active_workers"`
	AverageEPS      int64                    `json:"average_eps"`
	PacketLossRate  float64                  `json:"packet_loss_rate"`
	WorkerMetrics   map[int]WorkerMetrics    `json:"worker_metrics"`
	SystemMetrics   SystemMetrics            `json:"system_metrics"`
	LastUpdate      time.Time                `json:"last_update"`
}

// SystemMetrics - ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­
type SystemMetrics struct {
	CPUUsagePercent    float64 `json:"cpu_usage_percent"`
	MemoryUsageMB      float64 `json:"memory_usage_mb"`
	GoroutineCount     int     `json:"goroutine_count"`
	GCPauseMs          float64 `json:"gc_pause_ms"`
	NetworkTxMBps      float64 `json:"network_tx_mbps"`
}

// WorkerPool - ê³ ì„±ëŠ¥ ì›Œì»¤ í’€ ê´€ë¦¬ì (í”„ë¡œíŒŒì¼ ê¸°ë°˜)
type WorkerPool struct {
	// ì›Œì»¤ ê´€ë¦¬
	workers         []*UDPWorker
	workerCount     int
	targetHost      string
	
	// í”„ë¡œíŒŒì¼ ì„¤ì •
	profile         *config.EPSProfile
	
	// ë©”íŠ¸ë¦­ ìˆ˜ì§‘
	metricsChannel  chan WorkerMetrics
	poolMetrics     atomic.Value  // WorkerPoolMetrics ì €ì¥
	
	// ìƒíƒœ ê´€ë¦¬
	isRunning       atomic.Bool
	startTime       time.Time
	
	// ë™ê¸°í™”
	ctx             context.Context
	cancel          context.CancelFunc
	wg              sync.WaitGroup
	mutex           sync.RWMutex
	
	// ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
	epsHistory      []int64    // EPS ì´ë ¥ (ìµœê·¼ 300ì´ˆ)
	historyIndex    int
	lastHistoryTime time.Time
	
	// ìë™ ì¡°ì ˆ ê¸°ëŠ¥
	autoTuning      bool
	targetEPS       int64
	tuningEnabled   atomic.Bool
}

// NewWorkerPool - ì›Œì»¤ í’€ ìƒì„± ë° ì´ˆê¸°í™”
func NewWorkerPool(targetHost string) *WorkerPool {
	ctx, cancel := context.WithCancel(context.Background())
	
	// ê¸°ë³¸ í”„ë¡œíŒŒì¼ (4M)
	defaultProfile := config.EPSProfiles["4m"]
	
	pool := &WorkerPool{
		workers:        make([]*UDPWorker, 0, TOTAL_WORKERS),
		targetHost:     targetHost,
		profile:        defaultProfile,
		metricsChannel: make(chan WorkerMetrics, TOTAL_WORKERS*2), // ë²„í¼ í¬ê¸° ì—¬ìœ 
		ctx:            ctx,
		cancel:         cancel,
		epsHistory:     make([]int64, 300), // 5ë¶„ê°„ ì´ë ¥
		targetEPS:      int64(defaultProfile.TargetEPS),
		autoTuning:     true,
	}
	
	// ì´ˆê¸° ë©”íŠ¸ë¦­ ì„¤ì •
	pool.poolMetrics.Store(WorkerPoolMetrics{
		WorkerMetrics: make(map[int]WorkerMetrics),
		LastUpdate:    time.Now(),
	})
	
	return pool
}

// NewWorkerPoolWithProfile - í”„ë¡œíŒŒì¼ ê¸°ë°˜ ì›Œì»¤ í’€ ìƒì„±
func NewWorkerPoolWithProfile(targetHost string, profile *config.EPSProfile) *WorkerPool {
	ctx, cancel := context.WithCancel(context.Background())
	
	// í”„ë¡œíŒŒì¼ ê¸°ë°˜ ìµœì í™” ì ìš©
	if profile.GOGC > 0 {
		debug.SetGCPercent(profile.GOGC)
	}
	if profile.MemoryLimit > 0 {
		debug.SetMemoryLimit(profile.MemoryLimit)
	}
	
	pool := &WorkerPool{
		workers:        make([]*UDPWorker, 0, profile.WorkerCount),
		targetHost:     targetHost,
		profile:        profile,
		metricsChannel: make(chan WorkerMetrics, profile.WorkerCount*2),
		ctx:            ctx,
		cancel:         cancel,
		epsHistory:     make([]int64, 300),
		targetEPS:      int64(profile.TargetEPS),
		autoTuning:     false, // í”„ë¡œíŒŒì¼ ëª¨ë“œì—ì„œëŠ” ìë™ íŠœë‹ ë¹„í™œì„±í™”
	}
	
	// ì´ˆê¸° ë©”íŠ¸ë¦­ ì„¤ì •
	pool.poolMetrics.Store(WorkerPoolMetrics{
		WorkerMetrics: make(map[int]WorkerMetrics),
		LastUpdate:    time.Now(),
	})
	
	return pool
}

// Initialize - í”„ë¡œíŒŒì¼ ê¸°ë°˜ ì›Œì»¤ ì´ˆê¸°í™”
func (wp *WorkerPool) Initialize() error {
	wp.mutex.Lock()
	defer wp.mutex.Unlock()
	
	if wp.isRunning.Load() {
		return fmt.Errorf("ì›Œì»¤ í’€ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
	}
	
	// í”„ë¡œíŒŒì¼ì— ë”°ë¥¸ ì›Œì»¤ ìˆ˜ ê²°ì •
	workerCount := wp.profile.WorkerCount
	if workerCount > TOTAL_WORKERS {
		workerCount = TOTAL_WORKERS
	}
	
	// ì›Œì»¤ ìƒì„±
	for i := 0; i < workerCount; i++ {
		workerID := i + 1
		port := FIRST_PORT + i
		
		// í”„ë¡œíŒŒì¼ ì„¤ì •ìœ¼ë¡œ ì›Œì»¤ ìƒì„±
		worker, err := NewUDPWorkerWithConfig(workerID, port, wp.targetHost, wp.metricsChannel, 
			wp.profile.BatchSize, wp.profile.TickerInterval)
		if err != nil {
			return fmt.Errorf("ì›Œì»¤ %d ìƒì„± ì‹¤íŒ¨: %v", workerID, err)
		}
		
		// ë²„í¼ í¬ê¸° ì„¤ì •
		if wp.profile.SendBufferSize > 0 {
			worker.SetBufferSizes(wp.profile.SendBufferSize*1024, wp.profile.ReceiveBufferSize*1024)
		}
		
		wp.workers = append(wp.workers, worker)
	}
	
	wp.workerCount = len(wp.workers)
	fmt.Printf("âœ“ %s í”„ë¡œíŒŒì¼: %dê°œ ì›Œì»¤ ì´ˆê¸°í™” ì™„ë£Œ (í¬íŠ¸ %d-%d)\n", 
		wp.profile.Name, wp.workerCount, FIRST_PORT, FIRST_PORT+wp.workerCount-1)
	fmt.Printf("  ëª©í‘œ EPS: %s, ë°°ì¹˜ í¬ê¸°: %d, íƒ€ì´ë¨¸: %dÎ¼s\n",
		formatNumber(int64(wp.profile.TargetEPS)), wp.profile.BatchSize, wp.profile.TickerInterval)
	
	return nil
}

// Start - ì›Œì»¤ í’€ ì‹œì‘ (400ë§Œ EPS ë‹¬ì„± ì‹œì‘)
func (wp *WorkerPool) Start() error {
	if !wp.isRunning.CompareAndSwap(false, true) {
		return fmt.Errorf("ì›Œì»¤ í’€ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
	}
	
	wp.startTime = time.Now()
	
	// ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì‹œì‘
	wp.wg.Add(1)
	go wp.metricsAggregator()
	
	// ìë™ íŠœë‹ ì‹œì‘
	if wp.autoTuning {
		wp.wg.Add(1)
		go wp.autoTuner()
	}
	
	// ëª¨ë“  ì›Œì»¤ ì‹œì‘
	for i, worker := range wp.workers {
		wp.wg.Add(1)
		go func(w *UDPWorker, index int) {
			defer wp.wg.Done()
			
			err := w.Start(wp.ctx)
			if err != nil {
				fmt.Printf("ì›Œì»¤ %d ì‹¤í–‰ ì‹¤íŒ¨: %v\n", w.ID, err)
			}
		}(worker, i)
		
		// ì›Œì»¤ ì‹œì‘ ê°„ê²© (ë¦¬ì†ŒìŠ¤ ê²½í•© ë°©ì§€)
		time.Sleep(time.Millisecond * 10)
	}
	
	fmt.Printf("ğŸš€ ì›Œì»¤ í’€ ì‹œì‘: %dê°œ ì›Œì»¤ ì‹¤í–‰ ì¤‘ (ëª©í‘œ: %s EPS)\n", wp.workerCount, formatNumber(wp.targetEPS))
	
	return nil
}

// metricsAggregator - ì›Œì»¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì§‘ê³„
func (wp *WorkerPool) metricsAggregator() {
	defer wp.wg.Done()
	
	ticker := time.NewTicker(time.Second)
	defer ticker.Stop()
	
	workerMetricsMap := make(map[int]WorkerMetrics)
	
	for {
		select {
		case <-wp.ctx.Done():
			return
		case <-ticker.C:
			// ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
			sysMetrics := wp.collectSystemMetrics()
			
			// ì›Œì»¤ ë©”íŠ¸ë¦­ ì§‘ê³„
			var totalEPS, totalSent, totalErrors int64
			var activeWorkers int
			var totalPacketLoss float64
			
			for _, worker := range wp.workers {
				if worker.IsRunning() {
					activeWorkers++
					totalEPS += worker.GetCurrentEPS()
					totalSent += worker.GetTotalSent()
				}
			}
			
			// í‰ê·  EPS ê³„ì‚°
			var averageEPS int64
			if activeWorkers > 0 {
				averageEPS = totalEPS / int64(activeWorkers)
			}
			
			// EPS ì´ë ¥ ì—…ë°ì´íŠ¸
			wp.updateEPSHistory(totalEPS)
			
			// í’€ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
			poolMetrics := WorkerPoolMetrics{
				TotalEPS:       totalEPS,
				TotalSent:      totalSent,
				TotalErrors:    totalErrors,
				ActiveWorkers:  activeWorkers,
				AverageEPS:     averageEPS,
				PacketLossRate: totalPacketLoss / float64(activeWorkers),
				WorkerMetrics:  workerMetricsMap,
				SystemMetrics:  sysMetrics,
				LastUpdate:     time.Now(),
			}
			
			wp.poolMetrics.Store(poolMetrics)
			
			// ì„±ëŠ¥ ë¡œê·¸ ì¶œë ¥ (1ë¶„ë§ˆë‹¤)
			if time.Since(wp.startTime).Seconds() > 0 && int(time.Since(wp.startTime).Seconds())%60 == 0 {
				wp.printPerformanceLog(poolMetrics)
			}
			
		case workerMetric := <-wp.metricsChannel:
			workerMetricsMap[workerMetric.WorkerID] = workerMetric
		}
	}
}

// collectSystemMetrics - ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
func (wp *WorkerPool) collectSystemMetrics() SystemMetrics {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	
	return SystemMetrics{
		CPUUsagePercent: wp.getCPUUsage(),
		MemoryUsageMB:   float64(m.Alloc) / 1024 / 1024,
		GoroutineCount:  runtime.NumGoroutine(),
		GCPauseMs:      float64(m.PauseNs[(m.NumGC+255)%256]) / 1000000,
	}
}

func (wp *WorkerPool) getCPUUsage() float64 {
	// ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ CPU ì‚¬ìš©ë¥  ì¸¡ì •ì´ í•„ìš”
	// ì—¬ê¸°ì„œëŠ” ì›Œì»¤ ìˆ˜ì™€ ëª©í‘œ ë‹¬ì„±ë¥  ê¸°ë°˜ ì¶”ì •
	currentMetrics := wp.GetMetrics()
	targetAchievementRate := float64(currentMetrics.TotalEPS) / float64(wp.targetEPS)
	
	// ì¶”ì • CPU ì‚¬ìš©ë¥  = ëª©í‘œ ë‹¬ì„±ë¥  * 75% (PRD ëª©í‘œ CPU ì‚¬ìš©ë¥ )
	return targetAchievementRate * 75.0
}

// updateEPSHistory - EPS ì´ë ¥ ì—…ë°ì´íŠ¸
func (wp *WorkerPool) updateEPSHistory(currentEPS int64) {
	now := time.Now()
	if now.Sub(wp.lastHistoryTime) >= time.Second {
		wp.epsHistory[wp.historyIndex] = currentEPS
		wp.historyIndex = (wp.historyIndex + 1) % len(wp.epsHistory)
		wp.lastHistoryTime = now
	}
}

// autoTuner - ìë™ ì„±ëŠ¥ íŠœë‹ (ì‹¤í—˜ì  ê¸°ëŠ¥)
func (wp *WorkerPool) autoTuner() {
	defer wp.wg.Done()
	
	if !wp.tuningEnabled.Load() {
		return
	}
	
	ticker := time.NewTicker(time.Second * 30) // 30ì´ˆë§ˆë‹¤ íŠœë‹
	defer ticker.Stop()
	
	for {
		select {
		case <-wp.ctx.Done():
			return
		case <-ticker.C:
			metrics := wp.GetMetrics()
			
			// ëª©í‘œ EPSì˜ 95% ë¯¸ë§Œì¸ ê²½ìš° ìµœì í™” ì‹œë„
			if metrics.TotalEPS < (wp.targetEPS * 95 / 100) {
				wp.performAutoTuning(metrics)
			}
		}
	}
}

func (wp *WorkerPool) performAutoTuning(metrics WorkerPoolMetrics) {
	// CPU ì‚¬ìš©ë¥ ì´ ë‚®ìœ¼ë©´ ì›Œì»¤ ì¦ê°€ ì‹œë„
	if metrics.SystemMetrics.CPUUsagePercent < 60 {
		fmt.Printf("ğŸ”§ ìë™ íŠœë‹: CPU ì‚¬ìš©ë¥  ë‚®ìŒ (%.1f%%), ì„±ëŠ¥ í–¥ìƒ ì‹œë„\n", 
			metrics.SystemMetrics.CPUUsagePercent)
	}
	
	// ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìœ¼ë©´ GC ê°•ì œ ì‹¤í–‰
	if metrics.SystemMetrics.MemoryUsageMB > 10*1024 { // 10GB
		runtime.GC()
		fmt.Printf("ğŸ”§ ìë™ íŠœë‹: ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰ (%.1f MB)\n", 
			metrics.SystemMetrics.MemoryUsageMB)
	}
}

// printPerformanceLog - ì„±ëŠ¥ ë¡œê·¸ ì¶œë ¥
func (wp *WorkerPool) printPerformanceLog(metrics WorkerPoolMetrics) {
	duration := time.Since(wp.startTime)
	targetAchievement := float64(metrics.TotalEPS) / float64(wp.targetEPS) * 100
	
	fmt.Printf("ğŸ“Š ì„±ëŠ¥ ë¦¬í¬íŠ¸ [%s ê²½ê³¼]\n", duration.Round(time.Second))
	fmt.Printf("   í˜„ì¬ EPS: %s / ëª©í‘œ: %s (%.1f%%)\n", 
		formatNumber(metrics.TotalEPS), formatNumber(wp.targetEPS), targetAchievement)
	fmt.Printf("   í™œì„± ì›Œì»¤: %d/%d\n", metrics.ActiveWorkers, wp.workerCount)
	fmt.Printf("   ì´ ì „ì†¡: %s logs\n", formatNumber(metrics.TotalSent))
	fmt.Printf("   CPU: %.1f%% | ë©”ëª¨ë¦¬: %.1f MB | ê³ ë£¨í‹´: %d\n",
		metrics.SystemMetrics.CPUUsagePercent,
		metrics.SystemMetrics.MemoryUsageMB,
		metrics.SystemMetrics.GoroutineCount)
	fmt.Printf("   íŒ¨í‚· ì†ì‹¤ë¥ : %.2f%%\n", metrics.PacketLossRate)
	fmt.Println("   " + strings.Repeat("=", 60))
}

// Stop - ì›Œì»¤ í’€ ì •ì§€
func (wp *WorkerPool) Stop() error {
	if !wp.isRunning.CompareAndSwap(true, false) {
		return fmt.Errorf("ì›Œì»¤ í’€ì´ ì‹¤í–‰ë˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤")
	}
	
	fmt.Printf("ğŸ›‘ ì›Œì»¤ í’€ ì •ì§€ ì‹œì‘...\n")
	
	// ì·¨ì†Œ ì‹ í˜¸ ì „ì†¡
	wp.cancel()
	
	// ëª¨ë“  ì›Œì»¤ ì •ì§€
	for _, worker := range wp.workers {
		err := worker.Stop()
		if err != nil {
			fmt.Printf("ì›Œì»¤ %d ì •ì§€ ì‹¤íŒ¨: %v\n", worker.ID, err)
		}
	}
	
	// ê³ ë£¨í‹´ ì •ë¦¬ ëŒ€ê¸°
	wp.wg.Wait()
	
	// ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸
	finalMetrics := wp.GetMetrics()
	duration := time.Since(wp.startTime)
	
	fmt.Printf("ğŸ ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸:\n")
	fmt.Printf("   ì‹¤í–‰ ì‹œê°„: %s\n", duration.Round(time.Second))
	fmt.Printf("   ì´ ì „ì†¡ ë¡œê·¸: %s\n", formatNumber(finalMetrics.TotalSent))
	fmt.Printf("   í‰ê·  EPS: %s\n", formatNumber(finalMetrics.TotalSent/int64(duration.Seconds())))
	fmt.Printf("   ëª©í‘œ ë‹¬ì„±ë¥ : %.1f%%\n", 
		float64(finalMetrics.TotalEPS)/float64(wp.targetEPS)*100)
	
	return nil
}

// GetMetrics - í˜„ì¬ ë©”íŠ¸ë¦­ ë°˜í™˜
func (wp *WorkerPool) GetMetrics() WorkerPoolMetrics {
	if value := wp.poolMetrics.Load(); value != nil {
		return value.(WorkerPoolMetrics)
	}
	
	return WorkerPoolMetrics{
		WorkerMetrics: make(map[int]WorkerMetrics),
		LastUpdate:    time.Now(),
	}
}

// GetEPSHistory - EPS ì´ë ¥ ë°˜í™˜ (ëª¨ë‹ˆí„°ë§ìš©)
func (wp *WorkerPool) GetEPSHistory() []int64 {
	wp.mutex.RLock()
	defer wp.mutex.RUnlock()
	
	history := make([]int64, len(wp.epsHistory))
	copy(history, wp.epsHistory)
	return history
}

// IsRunning - ì‹¤í–‰ ìƒíƒœ í™•ì¸
func (wp *WorkerPool) IsRunning() bool {
	return wp.isRunning.Load()
}

// GetWorkerCount - ì›Œì»¤ ìˆ˜ ë°˜í™˜
func (wp *WorkerPool) GetWorkerCount() int {
	return wp.workerCount
}

// GetProfile - í˜„ì¬ í”„ë¡œíŒŒì¼ ë°˜í™˜
func (wp *WorkerPool) GetProfile() *config.EPSProfile {
	return wp.profile
}

// SetProfile - í”„ë¡œíŒŒì¼ ë³€ê²½ (ì¬ì‹œì‘ í•„ìš”)
func (wp *WorkerPool) SetProfile(profile *config.EPSProfile) error {
	if wp.isRunning.Load() {
		return fmt.Errorf("ì›Œì»¤ í’€ ì‹¤í–‰ ì¤‘ì—ëŠ” í”„ë¡œíŒŒì¼ì„ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
	}
	
	wp.profile = profile
	wp.targetEPS = int64(profile.TargetEPS)
	
	// í”„ë¡œíŒŒì¼ ê¸°ë°˜ ì‹œìŠ¤í…œ ìµœì í™”
	if profile.GOGC > 0 {
		debug.SetGCPercent(profile.GOGC)
	}
	if profile.MemoryLimit > 0 {
		debug.SetMemoryLimit(profile.MemoryLimit)
	}
	
	fmt.Printf("í”„ë¡œíŒŒì¼ ë³€ê²½: %s (%s)\n", profile.Name, profile.Description)
	return nil
}

// EnableAutoTuning - ìë™ íŠœë‹ í™œì„±í™”/ë¹„í™œì„±í™”
func (wp *WorkerPool) EnableAutoTuning(enabled bool) {
	wp.tuningEnabled.Store(enabled)
	fmt.Printf("ìë™ íŠœë‹: %t\n", enabled)
}

// formatNumber - ìˆ«ì í¬ë§·íŒ… (ê°€ë…ì„± í–¥ìƒ)
func formatNumber(n int64) string {
	if n >= 1000000 {
		return fmt.Sprintf("%.2fM", float64(n)/1000000)
	} else if n >= 1000 {
		return fmt.Sprintf("%.1fK", float64(n)/1000)
	}
	return fmt.Sprintf("%d", n)
}

// strings.Repeat êµ¬í˜„
func repeatString(s string, count int) string {
	result := ""
	for i := 0; i < count; i++ {
		result += s
	}
	return result
}